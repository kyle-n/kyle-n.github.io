import { c as create_ssr_component } from "./ssr.js";
const metadata = {
  "layout": "post",
  "title": "WWDC Wishlist- A free large-language model built into iOS and macOS",
  "date": "2023-04-24T00:00:00.000Z",
  "keywords": "apple, swift",
  "image": "wwdc23.jpg"
};
const Wwdc_23_wishlist_free_local_chatgpt = create_ssr_component(($$result, $$props, $$bindings, slots) => {
  return `<p data-svelte-h="svelte-1tp09oz">As an Apple developer, one thing I would love to see at WWDC is a free large-language model built into iOS and macOS. Ship GPT-whatever with the OS and let third-party apps call into it for free. It doesn’t have to be as complex as ChatGPT, just something that can run on Apple Silicon.</p> <p data-svelte-h="svelte-lo7my">This would be good for a couple reasons.</p> <p data-svelte-h="svelte-iiikz7">First, it would be incredibly useful for us third-party developers. I am currently writing a <a href="https://www.nazariosoftware.com/2023/04/14/log-in-seamlessly-with-autofill-email-codes.html" rel="nofollow">web extension for autofilling email login codes</a>. The app has an algorithm that finds login codes in the text of an email. It works pretty well, but you know what’s even better? ChatGPT. ChatGPT finds the code every single time.[^1]</p> <p data-svelte-h="svelte-1p444v1">[^1]: I also tried machine learning, but it is not something I have a lot of expertise with. <a href="https://www.tensorflow.org/js" rel="nofollow">Tensorflow.js</a> with the <a href="https://github.com/tensorflow/tfjs-models/tree/master/qna" rel="nofollow">Q&amp;A model</a> was slow and had a terrible accuracy rate.</p> <p data-svelte-h="svelte-zmtlbj">Unfortunately, ChatGPT is not feasible in my app. Using their API would be an ongoing cost, which would require an ongoing subscription. Additionally, I don’t want to send users’ emails to OpenAI.</p> <p data-svelte-h="svelte-o9ot7r">I could ship an LLM like <a href="https://github.com/ggerganov/llama.cpp" rel="nofollow">llama.cpp</a> with my app but LLMs are, at the moment, too large. I don’t want to include a multi-gigabyte model file in a web extension. Better for Apple to put it in the OS once and share it across all apps.</p> <p data-svelte-h="svelte-1ff1uzv">Apple could also implement a locally run LLM more efficiently than any third party. Last year, they patched CoreML and <a href="https://arstechnica.com/information-technology/2022/12/apple-slices-its-ai-image-synthesis-times-in-half-with-new-stable-diffusion-fix/" rel="nofollow">made Stable Diffusion run twice as fast</a>.</p> <p data-svelte-h="svelte-189ftkm">A free and locally run LLM would be good for Apple users’ privacy as well. If I were Tim Cook, I might be concerned about lots of apps on my platform using LLM services that keep user data to train their models. If Apple provided a free, local, private LLM for developers instead, it would encourage third-party developers to not use other services.</p> <p data-svelte-h="svelte-1m53d4">It would even encourage developers to create only for Apple platforms. This theoretical LLM, if it ever exists, will be enabled by Apple Silicon. It will be possible only because of hardware you can’t get on other platforms.</p> <p data-svelte-h="svelte-1xzwzf1">So, fingers crossed. Here’s hoping for something great this year at WWDC with generative AI.</p>`;
});
export {
  Wwdc_23_wishlist_free_local_chatgpt as default,
  metadata
};
